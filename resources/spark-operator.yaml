apiVersion: "sparkoperator.k8s.io/v1beta2"
#kind: SparkApplication
kind: ScheduledSparkApplication
metadata:
  name: siqmiraj
  namespace: spark-jobs
spec:
  schedule: "@every 30s"
  concurrencyPolicy: Forbid
  successfulRunHistoryLimit: 10
  failedRunHistoryLimit: 1
  template:
    type: Scala
    mode: cluster
    image: "mirajgodha/spark:latest"
    #imagePullPolicy: IfNotPresent
    imagePullPolicy: Always
    mainClass: com.miraj.SparkStreamingFromDirectory
    mainApplicationFile: "hdfs://siq02/tmp/miraj/siq_2.12-6.0.0.jar"
    arguments: ['miraj', 'miraj1', '1580803200']
    sparkVersion: "2.4.5"
    restartPolicy:
      type: Never
    hadoopConfigMap: spark-hadoop
    driver:
      cores: 1
      coreLimit: "1200m"
      memory: "2g"
      configMaps:
        - name: prometheus-file
          path: /opt/spark/work-dir/config
        - name: metrics-file
          path: /etc/metrics/conf
      labels:
        version: 2.4.5
      serviceAccount: spark-sa
      securityContext:
        runAsUser: 45721
    executor:
      cores: 2
      instances: 1
      memory: "2g"
      configMaps:
        - name: prometheus-file
          path: /opt/spark/work-dir/config
        - name: metrics-file
          path: /etc/metrics/conf
      labels:
        version: 2.4.5
      serviceAccount: spark-sa
      securityContext:
        runAsUser: 45721
    monitoring:
      exposeDriverMetrics: true
      exposeExecutorMetrics: true
      #metricsPropertiesFile: "/opt/spark/work-dir/config1/metrics.properties"
      prometheus:
        jmxExporterJar: "/opt/spark/examples/jars/jmx_prometheus_javaagent-0.11.0.jar"
        port: 7079
        configFile: "/opt/spark/work-dir/config/prometheus.yaml"
    hadoopConf:
      "dfs.nameservices": siq02
      "dfs.ha.namenodes.siq02": "nn0,nn1"
      "dfs.namenode.rpc-address.siq02.nn0": "siq02-mst-01.cloud.in.guavus.com:8020"
      "dfs.namenode.rpc-address.siq02.nn1": "siq02-mst-02.cloud.in.guavus.com:8020"
      "dfs.client.failover.proxy.provider.siq02": "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"
    sparkConf:
      "spark.ui.port": "4049"
      "spark.eventLog.enabled": "true"